# Property-Based Testing Troubleshooting Guide

This guide helps you understand, debug, and fix issues with property-based tests in CodeArchaeologist.

## Table of Contents

1. [Understanding Property-Based Testing](#understanding-property-based-testing)
2. [Common Property Test Failures](#common-property-test-failures)
3. [Hypothesis Shrinking Explained](#hypothesis-shrinking-explained)
4. [Debugging Strategies](#debugging-strategies)
5. [FAQ](#faq)

---

## Understanding Property-Based Testing

### What is Property-Based Testing?

Property-based testing (PBT) verifies that certain properties hold true for ALL valid inputs, not just specific examples.

**Traditional Unit Test:**
```python
def test_add_task():
    todo_list = TodoList()
    todo_list.add("Buy milk")
    assert len(todo_list) == 1
```

**Property-Based Test:**
```python
@given(task_description=st.text(min_size=1))
def test_add_task_increases_length(task_description):
    """For ANY non-empty task, adding it increases length by 1"""
    todo_list = TodoList()
    initial_length = len(todo_list)
    todo_list.add(task_description)
    assert len(todo_list) == initial_length + 1
```

The property test runs 100+ times with different inputs automatically generated by Hypothesis.

### Why Use Property-Based Testing?

**Benefits:**
- Finds edge cases you didn't think of
- Tests behavior across the entire input space
- Catches bugs that slip through example-based tests
- Documents system invariants clearly

**Example:** A property test might discover that your URL validator fails on:
- URLs with null bytes: `http://example.com\x00`
- URLs with unicode characters: `http://例え.jp`
- URLs with unusual ports: `http://example.com:99999`

---

## Common Property Test Failures

### 1. Generator Produces Invalid Inputs

**Symptom:**
```
Falsifying example: test_parse_dependency(
    dependency_spec='!!invalid!!'
)
AssertionError: Expected valid dependency, got error
```

**Cause:** Your generator is creating inputs outside the valid domain.

**Solution:** Constrain your generator to only produce valid inputs:

```python
# ❌ BAD: Generates invalid specs
@composite
def generate_dependency_spec(draw):
    return draw(st.text())  # Can generate anything!

# ✅ GOOD: Only generates valid specs
@composite
def generate_dependency_spec(draw):
    name = draw(st.text(
        alphabet=st.characters(whitelist_categories=('Ll', 'Lu', 'Nd')),
        min_size=1,
        max_size=50
    ))
    version = draw(st.text(regex=r'\d+\.\d+\.\d+'))
    return f"{name}=={version}"
```

### 2. Test Assumes Specific Input Characteristics

**Symptom:**
```
Falsifying example: test_url_validation(
    url='http://a.b'  # Minimal valid URL
)
AssertionError: Expected URL to have path component
```

**Cause:** Your test assumes URLs have certain features (like paths), but the property should hold for ALL valid URLs.

**Solution:** Either:
1. Fix the test to handle all valid inputs
2. Constrain the generator if the assumption is correct

```python
# ❌ BAD: Assumes URL has path
def test_url_validation(url):
    parsed = parse_url(url)
    assert parsed.path.startswith('/')  # Fails for 'http://a.b'

# ✅ GOOD: Handles all valid URLs
def test_url_validation(url):
    parsed = parse_url(url)
    assert parsed.scheme in ['http', 'https', 'git', 'ssh']
    # Don't assume path exists
```

### 3. Flaky Tests Due to Randomness

**Symptom:** Test passes sometimes, fails other times with different examples.

**Cause:** Your code has non-deterministic behavior or race conditions.

**Solution:**
1. Use `@settings(derandomize=True)` to make tests deterministic
2. Fix the underlying non-determinism in your code
3. Use database transactions/rollbacks for isolation

```python
from hypothesis import settings

@settings(derandomize=True)  # Same random seed each run
@given(data=st.data())
def test_deterministic_behavior(data):
    # Test will use same random values each time
    pass
```

### 4. Test Times Out

**Symptom:**
```
Hypothesis: Test exceeded deadline of 5000ms
```

**Cause:** Your code is too slow, or you're doing expensive operations (like network calls).

**Solution:**
1. Mock expensive operations
2. Increase deadline for slow but correct tests
3. Optimize the code being tested

```python
from hypothesis import settings

@settings(deadline=10000)  # 10 seconds
@given(large_data=st.lists(st.integers(), min_size=1000))
def test_slow_operation(large_data):
    # Allowed to take longer
    process_large_dataset(large_data)
```

### 5. Database State Pollution

**Symptom:** Tests fail when run together but pass individually.

**Cause:** Tests are sharing database state.

**Solution:** Use pytest fixtures with proper cleanup:

```python
import pytest
from sqlalchemy.orm import Session

@pytest.fixture
def db_session():
    """Provide a transactional scope for tests"""
    session = Session()
    try:
        yield session
    finally:
        session.rollback()  # Undo all changes
        session.close()

@given(repo_data=generate_repository_metadata())
def test_with_database(db_session, repo_data):
    # Changes are rolled back after test
    repo = Repository(**repo_data)
    db_session.add(repo)
    db_session.commit()
```

### 6. Percentage/Sum Invariants Fail Due to Floating Point

**Symptom:**
```
Falsifying example: test_language_percentages(
    languages=[{'name': 'Python', 'percentage': 33.333333}]
)
AssertionError: Expected sum to be 100.0, got 99.999999
```

**Cause:** Floating point arithmetic is imprecise.

**Solution:** Use approximate equality:

```python
# ❌ BAD: Exact equality
assert sum(lang['percentage'] for lang in languages) == 100.0

# ✅ GOOD: Approximate equality
assert abs(sum(lang['percentage'] for lang in languages) - 100.0) < 0.01
```

---

## Hypothesis Shrinking Explained

### What is Shrinking?

When Hypothesis finds a failing test case, it automatically tries to find the **simplest** example that still fails. This is called "shrinking."

**Example:**

```python
@given(st.text())
def test_no_null_bytes(text):
    assert '\x00' not in text
```

If this test fails, Hypothesis might initially find:
```
Falsifying example: test_no_null_bytes(
    text='abc\x00def\x00ghi'  # Initial failure
)
```

After shrinking:
```
Falsifying example: test_no_null_bytes(
    text='\x00'  # Minimal failure
)
```

### Why Shrinking Matters

Minimal examples are easier to debug:
- Less noise to wade through
- Clearer understanding of the root cause
- Easier to reproduce manually

### Shrinking Process

1. **Find a failure:** Hypothesis generates random inputs until one fails
2. **Simplify:** Try smaller/simpler versions of the failing input
3. **Verify:** Check if the simpler version still fails
4. **Repeat:** Keep simplifying until no simpler failing example exists

### Controlling Shrinking

```python
from hypothesis import settings, Phase

# Disable shrinking (faster but less helpful failures)
@settings(phases=[Phase.generate])
@given(st.text())
def test_without_shrinking(text):
    pass

# Increase shrinking effort
@settings(max_shrinks=1000)  # Default is 100
@given(st.text())
def test_with_more_shrinking(text):
    pass
```

---

## Debugging Strategies

### Strategy 1: Add the Failing Example as a Unit Test

When a property test fails, immediately add it as a regression test:

```python
# Property test failed with this example
def test_regression_null_byte_in_url():
    """Regression test for property test failure"""
    url = 'http://example.com\x00'
    with pytest.raises(ValueError):
        validate_url(url)
```

Now you can debug this specific case without running the full property test.

### Strategy 2: Use `@example()` Decorator

Force Hypothesis to test specific examples:

```python
from hypothesis import given, example

@given(st.text())
@example('')  # Always test empty string
@example('\x00')  # Always test null byte
@example('a' * 10000)  # Always test long string
def test_with_examples(text):
    process_text(text)
```

### Strategy 3: Print Intermediate Values

Add logging to understand what's happening:

```python
import logging

@given(st.integers())
def test_with_logging(value):
    logging.info(f"Testing with value: {value}")
    result = process(value)
    logging.info(f"Result: {result}")
    assert result > 0
```

Run with: `pytest -v --log-cli-level=INFO`

### Strategy 4: Use `note()` for Hypothesis-Specific Logging

```python
from hypothesis import given, note

@given(st.integers())
def test_with_notes(value):
    note(f"Input value: {value}")
    result = process(value)
    note(f"Processed result: {result}")
    assert result > 0
```

Notes appear in the failure output automatically.

### Strategy 5: Reproduce with Specific Seed

Every Hypothesis run uses a random seed. To reproduce a failure:

```bash
# Hypothesis prints the seed when a test fails
pytest --hypothesis-seed=12345
```

### Strategy 6: Use `@reproduce_failure()`

When a test fails, Hypothesis prints a `@reproduce_failure()` decorator:

```python
from hypothesis import reproduce_failure

@reproduce_failure('6.92.0', b'AAAA...')  # Copied from failure output
@given(st.text())
def test_reproduce_exact_failure(text):
    # Will reproduce the exact same failure
    pass
```

### Strategy 7: Simplify the Property

If you can't figure out why a test is failing, simplify it:

```python
# Complex property (hard to debug)
@given(repo=generate_repository_metadata())
def test_complex(repo):
    result = analyze_repository(repo)
    assert result.languages_sum == 100.0
    assert result.file_count > 0
    assert all(issue.path for issue in result.issues)

# Simplified (easier to debug)
@given(repo=generate_repository_metadata())
def test_languages_sum_only(repo):
    result = analyze_repository(repo)
    assert result.languages_sum == 100.0
```

Test one property at a time to isolate the issue.

---

## FAQ

### Q: How many examples should I run?

**A:** Start with 100 (the default). Increase for critical code:
- Development: 100 examples (fast feedback)
- CI: 100 examples (reasonable time)
- Nightly: 1000 examples (thorough testing)
- Critical code: 10,000+ examples

```python
from hypothesis import settings

@settings(max_examples=1000)
@given(st.text())
def test_critical_code(text):
    pass
```

### Q: My property test is too slow. What should I do?

**A:** Several options:
1. Mock expensive operations (database, network, AI calls)
2. Use smaller generated data
3. Increase the deadline
4. Optimize the code being tested

```python
from hypothesis import settings
from unittest.mock import patch

@settings(deadline=None)  # No timeout
@patch('services.ai_engine.call_gemini')  # Mock AI
@given(code=st.text(max_size=100))  # Smaller inputs
def test_faster(mock_ai, code):
    mock_ai.return_value = "mocked response"
    analyze_code(code)
```

### Q: Should I use property tests or unit tests?

**A:** Use both! They complement each other:

**Unit Tests:**
- Specific examples
- Edge cases you know about
- Regression tests
- Fast execution

**Property Tests:**
- Universal behaviors
- Edge cases you don't know about
- Invariants and contracts
- Thorough coverage

### Q: How do I test stateful systems?

**A:** Use Hypothesis's stateful testing:

```python
from hypothesis.stateful import RuleBasedStateMachine, rule

class TodoListMachine(RuleBasedStateMachine):
    def __init__(self):
        super().__init__()
        self.todo_list = TodoList()
    
    @rule(task=st.text(min_size=1))
    def add_task(self, task):
        self.todo_list.add(task)
        assert len(self.todo_list) > 0
    
    @rule()
    def clear_all(self):
        self.todo_list.clear()
        assert len(self.todo_list) == 0

TestTodoList = TodoListMachine.TestCase
```

### Q: Can I use property tests with databases?

**A:** Yes! Use fixtures with transactions:

```python
@pytest.fixture
def db_session():
    session = Session()
    try:
        yield session
    finally:
        session.rollback()
        session.close()

@given(data=generate_repository_metadata())
def test_with_db(db_session, data):
    repo = Repository(**data)
    db_session.add(repo)
    db_session.flush()  # Don't commit
    
    retrieved = db_session.query(Repository).first()
    assert retrieved.url == repo.url
    # Rolled back automatically
```

### Q: What if my generator is too complex?

**A:** Break it into smaller composable generators:

```python
@composite
def generate_name(draw):
    return draw(st.text(
        alphabet=st.characters(whitelist_categories=('Ll', 'Lu')),
        min_size=1,
        max_size=50
    ))

@composite
def generate_version(draw):
    major = draw(st.integers(min_value=0, max_value=10))
    minor = draw(st.integers(min_value=0, max_value=20))
    patch = draw(st.integers(min_value=0, max_value=50))
    return f"{major}.{minor}.{patch}"

@composite
def generate_dependency(draw):
    name = draw(generate_name())
    version = draw(generate_version())
    return f"{name}=={version}"
```

### Q: How do I test error handling?

**A:** Generate both valid and invalid inputs:

```python
@given(url=st.one_of(
    generate_valid_url(),
    generate_invalid_url()
))
def test_url_handling(url):
    try:
        result = validate_url(url)
        # If we get here, URL was valid
        assert result.scheme in ['http', 'https']
    except ValueError:
        # If we get here, URL was invalid
        # This is expected for invalid URLs
        pass
```

### Q: Can I test async code?

**A:** Yes! Use pytest-asyncio:

```python
import pytest
from hypothesis import given

@pytest.mark.asyncio
@given(url=generate_git_url())
async def test_async_clone(url):
    result = await clone_repository(url)
    assert result.success
```

### Q: What's the difference between `assume()` and filtering?

**A:**

**`assume()`** - Skip examples that don't meet preconditions:
```python
from hypothesis import assume

@given(st.integers())
def test_positive_only(x):
    assume(x > 0)  # Skip non-positive values
    assert process(x) > 0
```

**Filtering** - Generate only valid examples:
```python
@given(st.integers().filter(lambda x: x > 0))
def test_positive_only(x):
    # x is always positive
    assert process(x) > 0
```

Use filtering when possible (more efficient). Use `assume()` when filtering is complex.

### Q: How do I handle test data that needs cleanup?

**A:** Use pytest fixtures:

```python
import pytest
import tempfile
import shutil

@pytest.fixture
def temp_repo_dir():
    """Create temporary directory for repository cloning"""
    temp_dir = tempfile.mkdtemp()
    try:
        yield temp_dir
    finally:
        shutil.rmtree(temp_dir)

@given(url=generate_git_url())
def test_with_cleanup(temp_repo_dir, url):
    # temp_repo_dir is cleaned up automatically
    clone_to_directory(url, temp_repo_dir)
```

---

## Additional Resources

- [Hypothesis Documentation](https://hypothesis.readthedocs.io/)
- [Property-Based Testing with Hypothesis](https://hypothesis.works/)
- [Hypothesis Examples](https://github.com/HypothesisWorks/hypothesis/tree/master/hypothesis-python/examples)
- [CodeArchaeologist Design Doc](.kiro/specs/phase3-property-testing/design.md)
- [CodeArchaeologist Requirements](.kiro/specs/phase3-property-testing/requirements.md)

---

## Getting Help

If you're stuck:

1. Check this guide for common issues
2. Review the Hypothesis documentation
3. Look at existing property tests in `backend/tests/test_properties_*.py`
4. Run with `--hypothesis-show-statistics` to see what Hypothesis is doing
5. Add `@example()` decorators to test specific cases
6. Simplify the property to isolate the issue
7. Ask for help in the project issues or discussions

Remember: Property tests are powerful but can be tricky. Start simple and build up complexity gradually!
